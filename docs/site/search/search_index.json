{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home The OOT Operator optionally builds and runs DriverContainers on Kubernetes clusters.","title":"Home"},{"location":"#home","text":"The OOT Operator optionally builds and runs DriverContainers on Kubernetes clusters.","title":"Home"},{"location":"developer/","text":"Design notes The OOT Operator brings a new Module CRD. Module represents an out-of-tree kernel module that must be inserted into the cluster nodes\u2019 kernel through a DriverContainer scheduled by a DaemonSet . The OOTO optionally builds and runs DriverContainers on the cluster. It picks the right DriverContainer image by leveraging kernel mappings in the Module CRD that describe which image should be used for which kernel. A kernel mapping maps either a literal kernel name or a regex with a kernel\u2019s node. This allows for more flexibility when targeting a set of kernels (e.g. \u201cfor Ubuntu nodes, build from that repository\u201d). Variables available at build time still reflect the actual kernel version. Module CRD apiVersion: ooto.sigs.k8s.io/v1beta1 kind: Module metadata: name: module-sample spec: build: # Reference instructions to build the module git: repository: https://github.com/vendor/driver ref: some-tag buildArgs: PARAM1: value1 PARAM2: value2 containerFile: Dockerfile (or something else) contextDir: some/path/in/the/sources to: imageName: image-registry.openshift-image-registry.svc:5000/driver:${KERNEL_VERSION} pushSecret: name: push-secret-name namespace: push-secret-namespace devicePlugin: # is a Container spec driverContainer: # is a Container spec kernelMappings: - literal: 5.16.11-200.fc35.x86_64 containerImage: quay.io/vendor/module-sample:fedora-5.16.11-200.fc35.x86_64 - literal: 5.4.0-1054-gke containerImage: quay.io/vendor/module-sample:ubuntu-5.4.0-1054-gke - regexp: '^.*\\-gke$' build: git: ref: gke selector: # top-level selector feature.node.kubernetes.io/cpu-cpuid.VMX: true status: conditions: - lastTransitionTime: \"2022-03-07T14:01:00Z\" status: False type: Progressing - lastTransitionTime: \"2022-03-07T14:01:01Z\" status: True type: Ready In-cluster builds Virtually any Kubernetes distribution ships with its own kernel. It is thus challenging for a kernel module vendor to make DriverContainer images available for all kernels available in all Kubernetes distributions out there (or even for a subset of them). The OOTO supports in-cluster builds of DriverContainer images when those are not made available by the vendor. The in-cluster build system is able to build an image from any Git repository that contains a Dockerfile . It can then push said image to a user-provided repository, optionally using authentication provided in a Kubernetes secret. Optional: on upstream Kubernetes, we may want to deploy an in-cluster registry to host in-cluster built images. On OCP , the build mechanism would be BuildConfig (maybe Shipwright in the future) and we can leverage the integrated in-cluster registry. Unloading modules Users must provide a preStop lifecycle hook to their DriverContainer pod template to make sure that their module is unloaded when the DriverContainer pod exits. Security DriverContainer privileges By default, the OOTO would only grant the CAP_SYS_MODULE capability to DriverContainer DaemonSets . Kubernetes API privileges The OOTO would only be granted a limited set of Kubernetes API privileges: - Read and watch Nodes ; - Read and watch Modules , update their status; - Read, create, modify and watch DaemonSets ; - Read, create, modify and watch Build objects (from whatever system we agree on); - Read Secrets .","title":"Design notes"},{"location":"developer/#design-notes","text":"The OOT Operator brings a new Module CRD. Module represents an out-of-tree kernel module that must be inserted into the cluster nodes\u2019 kernel through a DriverContainer scheduled by a DaemonSet . The OOTO optionally builds and runs DriverContainers on the cluster. It picks the right DriverContainer image by leveraging kernel mappings in the Module CRD that describe which image should be used for which kernel. A kernel mapping maps either a literal kernel name or a regex with a kernel\u2019s node. This allows for more flexibility when targeting a set of kernels (e.g. \u201cfor Ubuntu nodes, build from that repository\u201d). Variables available at build time still reflect the actual kernel version.","title":"Design notes"},{"location":"developer/#module-crd","text":"apiVersion: ooto.sigs.k8s.io/v1beta1 kind: Module metadata: name: module-sample spec: build: # Reference instructions to build the module git: repository: https://github.com/vendor/driver ref: some-tag buildArgs: PARAM1: value1 PARAM2: value2 containerFile: Dockerfile (or something else) contextDir: some/path/in/the/sources to: imageName: image-registry.openshift-image-registry.svc:5000/driver:${KERNEL_VERSION} pushSecret: name: push-secret-name namespace: push-secret-namespace devicePlugin: # is a Container spec driverContainer: # is a Container spec kernelMappings: - literal: 5.16.11-200.fc35.x86_64 containerImage: quay.io/vendor/module-sample:fedora-5.16.11-200.fc35.x86_64 - literal: 5.4.0-1054-gke containerImage: quay.io/vendor/module-sample:ubuntu-5.4.0-1054-gke - regexp: '^.*\\-gke$' build: git: ref: gke selector: # top-level selector feature.node.kubernetes.io/cpu-cpuid.VMX: true status: conditions: - lastTransitionTime: \"2022-03-07T14:01:00Z\" status: False type: Progressing - lastTransitionTime: \"2022-03-07T14:01:01Z\" status: True type: Ready","title":"Module CRD"},{"location":"developer/#in-cluster-builds","text":"Virtually any Kubernetes distribution ships with its own kernel. It is thus challenging for a kernel module vendor to make DriverContainer images available for all kernels available in all Kubernetes distributions out there (or even for a subset of them). The OOTO supports in-cluster builds of DriverContainer images when those are not made available by the vendor. The in-cluster build system is able to build an image from any Git repository that contains a Dockerfile . It can then push said image to a user-provided repository, optionally using authentication provided in a Kubernetes secret. Optional: on upstream Kubernetes, we may want to deploy an in-cluster registry to host in-cluster built images. On OCP , the build mechanism would be BuildConfig (maybe Shipwright in the future) and we can leverage the integrated in-cluster registry.","title":"In-cluster builds"},{"location":"developer/#unloading-modules","text":"Users must provide a preStop lifecycle hook to their DriverContainer pod template to make sure that their module is unloaded when the DriverContainer pod exits.","title":"Unloading modules"},{"location":"developer/#security","text":"","title":"Security"},{"location":"developer/#drivercontainer-privileges","text":"By default, the OOTO would only grant the CAP_SYS_MODULE capability to DriverContainer DaemonSets .","title":"DriverContainer privileges"},{"location":"developer/#kubernetes-api-privileges","text":"The OOTO would only be granted a limited set of Kubernetes API privileges: - Read and watch Nodes ; - Read and watch Modules , update their status; - Read, create, modify and watch DaemonSets ; - Read, create, modify and watch Build objects (from whatever system we agree on); - Read Secrets .","title":"Kubernetes API privileges"},{"location":"developer/reconciliation_loops/","text":"Reconciliation loops Modules Each time a new Module is created, we need to find to which nodes it applies. A first filtering is performed using the .spec.selector field, then we go through the module\u2019s kernel mappings to find a container image that matches the node\u2019s kernel. We end up with a certain number of (kernel, image) pairs; for each of these pairs, there should be a DaemonSet . We first look for a DaemonSet that would already be targeting the same kernel and DriverContainer image (that data is stored in the DaemonSet \u2019s labels). If there is already such a DaemonSet , we update it, if needed. If there is not already a matching DaemonSet , we create it and set the Module as owner. When a Module is deleted, we have nothing to do: because we set it as owner of all DaemonSets , Kubernetes garbage collection will take care of deleting them. Nodes Each time a Node is created or updated, we must verify which Modules apply to it. The reconciliation process is similar to that of Module , except it is made in reverse. We first list all Modules and check which ones apply to the Node using each Module \u2019s .spec.selector field. We then iterate over the filtered Modules . For each Module , we try to find a suitable DriverContainer image by finding a kernelMapping that applies to the Node . We select the first occurrence and store this in a list of (kernel, DriverContainer image) pairs. After we have looped over all modules that apply to the Node , we reconcile DaemonSets corresponding to these (kernel, DriverContainer image) pairs like we did for Modules . DaemonSets Module and Node reconciliation loops create and update DaemonSets , but they never actively destroy them. A DaemonSet should be deleted when it runs a DriverContainer image for a kernel that no Node runs anymore. Because we use the DaemonSet \u2019s nodeSelector to target precise kernels, we can determine when a DaemonSet is outdated and does not target any Node in the cluster: that is when its desired replicas is set by Kubernetes to 0. We can thus watch all DaemonSets managed by the operator, and delete those that are not targeting any node.","title":"Reconciliation loops"},{"location":"developer/reconciliation_loops/#reconciliation-loops","text":"","title":"Reconciliation loops"},{"location":"developer/reconciliation_loops/#modules","text":"Each time a new Module is created, we need to find to which nodes it applies. A first filtering is performed using the .spec.selector field, then we go through the module\u2019s kernel mappings to find a container image that matches the node\u2019s kernel. We end up with a certain number of (kernel, image) pairs; for each of these pairs, there should be a DaemonSet . We first look for a DaemonSet that would already be targeting the same kernel and DriverContainer image (that data is stored in the DaemonSet \u2019s labels). If there is already such a DaemonSet , we update it, if needed. If there is not already a matching DaemonSet , we create it and set the Module as owner. When a Module is deleted, we have nothing to do: because we set it as owner of all DaemonSets , Kubernetes garbage collection will take care of deleting them.","title":"Modules"},{"location":"developer/reconciliation_loops/#nodes","text":"Each time a Node is created or updated, we must verify which Modules apply to it. The reconciliation process is similar to that of Module , except it is made in reverse. We first list all Modules and check which ones apply to the Node using each Module \u2019s .spec.selector field. We then iterate over the filtered Modules . For each Module , we try to find a suitable DriverContainer image by finding a kernelMapping that applies to the Node . We select the first occurrence and store this in a list of (kernel, DriverContainer image) pairs. After we have looped over all modules that apply to the Node , we reconcile DaemonSets corresponding to these (kernel, DriverContainer image) pairs like we did for Modules .","title":"Nodes"},{"location":"developer/reconciliation_loops/#daemonsets","text":"Module and Node reconciliation loops create and update DaemonSets , but they never actively destroy them. A DaemonSet should be deleted when it runs a DriverContainer image for a kernel that no Node runs anymore. Because we use the DaemonSet \u2019s nodeSelector to target precise kernels, we can determine when a DaemonSet is outdated and does not target any Node in the cluster: that is when its desired replicas is set by Kubernetes to 0. We can thus watch all DaemonSets managed by the operator, and delete those that are not targeting any node.","title":"DaemonSets"},{"location":"developer/use_cases/","text":"Use cases A new module is added to the cluster The operator lists all nodes required to build the module and looks at their kernels. It uses the Module \u2019s .spec.kernelMappings to create DaemonSets for each of those kernels. If an image for one of these DaemonSets requires an in-cluster build, it creates the build object (build system TBD). The DaemonSet owns the build object, so that it is deleted when the DaemonSet is deleted. Optional: we could watch the build object to rollout restart the DaemonSet as soon as the build is over and the image can be pulled. Module update: kernel $kernel should run a new DriverContainer image The OOTO would be triggered by an update to any Module instance. That reconciliation generates a list of DaemonSets that need to be reconciled. For each DaemonSet belonging to Module , we set the following labels: key value ooto.sigs.k8s.io/module-name $moduleName ooto.sigs.k8s.io/kernel-target $kernel We either create those DaemonSets (if they do not already exist) or update them (if a DaemonSet already exists with the same labels). If a DaemonSet already exists for $moduleName and $kernel , it is updated with the new DriverContainer image. If that image requires a build, we create the build object and wait for its completion before we modify the DaemonSet . A new node joins the cluster (or is modified) The OOTO lists all Modules and checks which ones should run on the node. For each Module that should run on the node (determined by the .spec.selector field), the operator tries to find a suitable DriverContainer image for the node\u2019s kernel. It creates a DaemonSet that runs that DriverContainer on all nodes running that kernel. A node leaves the cluster If that node was targeted by one of the DaemonSets , said DaemonSet \u2019s .status.desiredNumberScheduled is decremented. The operator watches changes to those objects; if that field reaches 0, the DaemonSet is deleted. A node\u2019s kernel is updated Depending on the Kubernetes distribution used, this is functionally equivalent to either: Deleting the node and creating a new one, or; Modifying an existing node. That use case is thus covered above. A node matches a Module\u2019s .spec.selector, but not any of its kernel mappings No DaemonSet is scheduled for this module and this kernel. The OOT kernel module will not be deployed on that node.","title":"Use cases"},{"location":"developer/use_cases/#use-cases","text":"","title":"Use cases"},{"location":"developer/use_cases/#a-new-module-is-added-to-the-cluster","text":"The operator lists all nodes required to build the module and looks at their kernels. It uses the Module \u2019s .spec.kernelMappings to create DaemonSets for each of those kernels. If an image for one of these DaemonSets requires an in-cluster build, it creates the build object (build system TBD). The DaemonSet owns the build object, so that it is deleted when the DaemonSet is deleted. Optional: we could watch the build object to rollout restart the DaemonSet as soon as the build is over and the image can be pulled.","title":"A new module is added to the cluster"},{"location":"developer/use_cases/#module-update-kernel-kernel-should-run-a-new-drivercontainer-image","text":"The OOTO would be triggered by an update to any Module instance. That reconciliation generates a list of DaemonSets that need to be reconciled. For each DaemonSet belonging to Module , we set the following labels: key value ooto.sigs.k8s.io/module-name $moduleName ooto.sigs.k8s.io/kernel-target $kernel We either create those DaemonSets (if they do not already exist) or update them (if a DaemonSet already exists with the same labels). If a DaemonSet already exists for $moduleName and $kernel , it is updated with the new DriverContainer image. If that image requires a build, we create the build object and wait for its completion before we modify the DaemonSet .","title":"Module update: kernel $kernel should run a new DriverContainer image"},{"location":"developer/use_cases/#a-new-node-joins-the-cluster-or-is-modified","text":"The OOTO lists all Modules and checks which ones should run on the node. For each Module that should run on the node (determined by the .spec.selector field), the operator tries to find a suitable DriverContainer image for the node\u2019s kernel. It creates a DaemonSet that runs that DriverContainer on all nodes running that kernel.","title":"A new node joins the cluster (or is modified)"},{"location":"developer/use_cases/#a-node-leaves-the-cluster","text":"If that node was targeted by one of the DaemonSets , said DaemonSet \u2019s .status.desiredNumberScheduled is decremented. The operator watches changes to those objects; if that field reaches 0, the DaemonSet is deleted.","title":"A node leaves the cluster"},{"location":"developer/use_cases/#a-nodes-kernel-is-updated","text":"Depending on the Kubernetes distribution used, this is functionally equivalent to either: Deleting the node and creating a new one, or; Modifying an existing node. That use case is thus covered above.","title":"A node\u2019s kernel is updated"},{"location":"developer/use_cases/#a-node-matches-a-modules-specselector-but-not-any-of-its-kernel-mappings","text":"No DaemonSet is scheduled for this module and this kernel. The OOT kernel module will not be deployed on that node.","title":"A node matches a Module\u2019s .spec.selector, but not any of its kernel mappings"}]}